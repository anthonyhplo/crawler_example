{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"crawler_yelp.ipynb","version":"0.3.2","provenance":[{"file_id":"1lgCI-CWuI3FRxTa_6hX6Z25Dgjal9GtF","timestamp":1568209455104},{"file_id":"1sgZEuMUn5swHAhtGcOvgSDIf1IUcwHGY","timestamp":1568202615239},{"file_id":"https://github.com/team-ant/crawler_example/blob/master/crawler_yelp.ipynb","timestamp":1568128149643}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"KQsTogoYKhbk","colab_type":"text"},"source":["## Crawler exercise (YELP)\n","#### by Anthony Lo and his team (https://github.com/team-ant/crawler_example)\n","\n","You will be using Python + BeautifulSoup to create a crawler that perform web scrapping on https://www.yelp.com. Through this exercise, you will know\n","- Send a http request from Python and get server's repsonse\n","- Preprocess scrapped data using BeautifulSoup\n","- Automate the crawler by simple Python coding\n","- Randomize user agent and set timer to minmize the risk of being blocked\n","- Export the data to a file for further analysis\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L6puhC00MJC_","colab_type":"text"},"source":["### Get all shop name in one page\n","Sample output:\n","1. Sugar\n","2. Camper’s\n","3. Mr & Mrs Fox\n","4. Shugetsu\n","5. 金峰靚靚粥麵\n","6. Public\n","7. Plat du Jour\n","8. Formosa Autumn\n","9. Nha Trang\n","10. Wah Cheong Congee & Noodle Shop"]},{"cell_type":"code","metadata":{"id":"V2XFy2gIm7TS","colab_type":"code","colab":{}},"source":["import requests\n","from bs4 import BeautifulSoup\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pCB7nb9Jkd3F","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f2e95SMncbDU","colab_type":"text"},"source":["### Get total number of pages\n","Write a function that return the toal pages of search results. Set a maximum number as the limit in the function.\n","\n","Example:\n","- if total pages = 6 and max = 3, return 3\n","- if total pages = 5 and max = 10, return 5\n","- if no result, return -1\n"]},{"cell_type":"code","metadata":{"id":"blG6vjUAkf6l","colab_type":"code","colab":{}},"source":["def get_total_pages:\n","  # code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DgWf4q8Ddm1z","colab_type":"text"},"source":["### Loop through pages and get a list of shop name and links\n","Given a search result page, crawl all shop names and links for maximum 3 pages\n","\n","Sample output:\n","<br>\n","[['shop1','https://abc.com'],['shop2','https://xxxagd.com']]\n","<br>\n","* Remember to setup randomized random agent and timer to prevent bloclking by server"]},{"cell_type":"code","metadata":{"id":"88H_9dxvu-Xv","colab_type":"code","outputId":"53a75961-9345-4ea3-f60e-db8ef27c70b5","executionInfo":{"status":"ok","timestamp":1568187211619,"user_tz":-480,"elapsed":7727,"user":{"displayName":"Eric Tai","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBLI-SGy8qrQhOysl7P9XEJ-DSijPF8_bKbxJaFyw=s64","userId":"05425123846616369489"}},"colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["# Install fake agent\n","!pip install fake-useragent"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting fake-useragent\n","  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n","Building wheels for collected packages: fake-useragent\n","  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fake-useragent: filename=fake_useragent-0.1.11-cp36-none-any.whl size=13485 sha256=399f3b2d5fcd06d7c0d31f6f049dc5c68ede0658683c1d8f2b14f3b122ab8870\n","  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n","Successfully built fake-useragent\n","Installing collected packages: fake-useragent\n","Successfully installed fake-useragent-0.1.11\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"05X1b0CdvOsX","colab_type":"code","colab":{}},"source":["from fake_useragent import UserAgent\n","import time\n","import random\n","ua = UserAgent()\n","ua.random"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ttlRW5KXm7Td","colab_type":"code","colab":{}},"source":["## get shop name and link\n","def get_shop_list(url, max_page=3):\n","    # code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ssz8KBuafAdm","colab_type":"text"},"source":["### Get reviews\n","Get reviews given a shop url, crawl maximum 3 pages of reviews\n","\n","sample output:\n","<br>\n","[['shop1', 3, 'it is just ok'], \n","['shop1',5,'the food is delicious!'']]"]},{"cell_type":"code","metadata":{"id":"NL8lHP33m7Th","colab_type":"code","colab":{}},"source":["def get_reviews(shop_name, url, max_page=5):\n","    # code here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hE6b-3mijnvD","colab_type":"text"},"source":["### Combine all together!\n","Given a search query, return shop list and their reviews. \n","\n","\n","Finally, export each review to a 3-columns csv files that contains \n","1. shop name\n","2. star of a review\n","3. review content"]},{"cell_type":"code","metadata":{"id":"XaC8NiRmkvmQ","colab_type":"code","colab":{}},"source":["# crawl all review and return a list of list \n","# code here"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Af1RnFbWm7Ts","colab_type":"code","colab":{}},"source":["# code here\n"],"execution_count":0,"outputs":[]}]}